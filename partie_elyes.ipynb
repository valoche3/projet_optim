{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3a8bf4-0cb6-4fca-92fc-ab62319e0338",
   "metadata": {},
   "source": [
    "## Projet d'optimisation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecccf546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from casadi import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae2cf9",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1ee91-f74b-4534-bb59-99920b73bb17",
   "metadata": {},
   "source": [
    "Le but du problème est de trouver l'allocation globale $x \\in \\mathbb{R}^m$ qui minimise le risque associé au rendement du portefeuille, i.e. la variance de cette allocation par rapport à la variation du prix des actifs $p \\in \\mathbb{R}^m$.\n",
    "\n",
    "La variance est l'équivalent d'un \"écart à la moyenne\" ; elle représente donc bien un indicateur du risque associé à un investissement. Ce risque prend en compte le lien entre la variation des prix et les allocations, d'où $x^T \\Sigma x$, où $\\Sigma$ est la matrice de covariance associé à $p$.\n",
    "\n",
    "La relation $\\bold{1}^T x = 1$ signifie simplement que l'on normalise l'allocation par souci de cohérence et de clarté ; pour une allocation donnée, le rendement représente le gain moyen associé à l'allocation, d'où $\\bar p^T x = r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8146c-8574-4d6a-864e-4309db37404a",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4373f-3240-46fb-9905-96283f5594f0",
   "metadata": {},
   "source": [
    "On souhaite minimiser le risque, donc on a\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "f : z & \\longmapsto & f(z) = x^T \\Sigma x\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "La variable $z$ dépend donc de $x$, mais aussi de $\\Sigma = \\left[ \\mathrm{Cov}(p_i, p_j) \\right]_{i,j \\in \\llbracket 1, m \\rrbracket}$ et donc de $p$. Le nombre de variables de décisions est donc $n = 2$, où chaque variable est un vecteur de $\\mathbb{R}^m$.\n",
    "\n",
    "L'énoncé contraint le problème par le biais de la condition de normalisation $\\bold{1}^T x = 1$ et celle de rendement $r = \\bar p^T x$ : on peut donc définir $$c_{eq,norm}(z) = \\bold{1}^T x - 1$$ et $$c_{eq,rend}(z) = \\bar p^T x - r$$ ce qui ajoute les contraintes égalité $c_{eq} : \\begin{cases} c_{eq,norm}(z) = 0 \\\\ c_{eq,rend}(z) = 0 \\end{cases}$ au problème de minimisation de $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c1e7d-7189-47a6-bff1-734f84c4c2bd",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17891231-c3c7-4dae-9a36-45602ca985fd",
   "metadata": {},
   "source": [
    "On ajoute une contrainte inégalité pour limiter les positions _short_ à un certain montant par allocation, afin de garantir un équilibre de marché, ne pas entraîner de trop fortes fluctuation des actifs et donc une variance trop élevée.\n",
    "\n",
    "On a donc dans ce problème une contrainte égalité (représentée par la fonction $c_{eq}$) et une contrainte inégalité.\n",
    "\n",
    "Or la contrainte inégalité comprend déjà une fonction $max$ ; on cherche donc à minimiser une fonction $f$ sous contrainte d'une limitation d'un maximum, ce qui risque de compliquer les calculs..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8aaf1-eeec-4950-81d1-886ed1497b7b",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c927196-71e0-4246-8123-e9faa0c72b85",
   "metadata": {},
   "source": [
    "$\\max(-x,\\bold{0})$ vérifie\n",
    "$\\forall x \\in \\mathbb{R}^m, \\max(-x,\\bold{0}) ≥ -x$ et $\\max(-x,\\bold{0}) ≥ 0$.\n",
    "\n",
    "En introduisant $s \\in \\mathbb{R}^m$ tel que $s ≥ -x$, $s ≥ 0$ et $\\bold{1}^T s ≤ s_M$, on a \n",
    "$$\\bold{1}^T \\max(-x,\\bold{0}) ≤ \\bold{1}^T s ≤ s_M$$\n",
    "En prenant $s$ le plus proche possible de $\\max(-x,\\bold{0})$, on peut reformuler de manière équivalente le problème de la question 2 :\n",
    "$$\\min f(z)$$\n",
    "sous les contraintes $c_{eq}(z) = 0$, $c_{in}(s) ≤ 0$ \n",
    "\n",
    "avec $\\begin{cases} s ≥ -x \\\\ s ≥ 0 \\end{cases}$ et $c_{in}(s) = \\bold{1}^T s - s_M$\n",
    "\n",
    "La variable $s$ est indirectement liée à $x$, et donc à $z$, puisqu'elle est choisie en fonction de $-x$ (on veut avoir $s = \\max(-x, \\bold{0})$ en se débarrassant du max)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a89a4a-e6f8-4851-b42c-1f745353ecbb",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b2778-1e94-4285-bde8-b72eae2a663e",
   "metadata": {},
   "source": [
    "Le problème $(4)$ est un problème d'optimisation sous contrainte avec une contrainte égalité affine.\n",
    "\n",
    "Une première méthode de résolution est d'utiliser les multiplicateurs de Lagrange, en calculant le lagrangien\n",
    "$$\\mathcal{L}(z, \\lambda) = f(z) + \\lambda^T c_{eq}(z)$$\n",
    "puis en  trouvant ses points stationnaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07dadc",
   "metadata": {},
   "source": [
    "On peut aussi penser aux conditions $(KKT)$ : en effet, une contrainte affine étant convexe, on a une équivalence entre respecter les conditions $(KKT)$ et être solution du problème d'optimisation sous contrainte affine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19d888-7ec9-459d-8c92-406cbaa6c773",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a6642",
   "metadata": {},
   "source": [
    "Pour résoudre numériquement le problème, on va utiliser les bibliothèques Scipy (avec son module _optimize_ et son solveur SLSQP) et Casadi (avec _ipopt_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b613f7-1d4e-4785-a85b-28c0e969d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des variables\n",
    "p1 = 0.05\n",
    "p2 = 0.15\n",
    "p3 = 0.3\n",
    "p = np.array([p1, p2, p3])\n",
    "rho = 0.1\n",
    "r = 0.1\n",
    "\n",
    "sig1 = 0.1\n",
    "sig2 = 0.3\n",
    "sig3 = 0.8\n",
    "sigma = np.array([[sig1**2, rho*sig1*sig2, 0], [rho*sig1*sig2, sig2**2, 0], [0, 0, sig3**2]])\n",
    "\n",
    "unit = np.array([1., 1., 1.])\n",
    "x0 = np.array([0.0, 0.0, 0.0])\n",
    "x_min = np.zeros_like(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f81494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de la fonction f et des contraintes\n",
    "\n",
    "def var(x):\n",
    "    return x.T @ sigma @ x\n",
    "\n",
    "contraintes = ({'type': 'eq', 'fun': lambda x:  unit.T@x -1},\n",
    "        {'type': 'eq', 'fun': lambda x: p.T@x - r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76aea81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "méthode SLSQP avec Scipy (r = 0.1)\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.017067775457853502\n",
      "            Iterations: 4\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 4\n",
      "Durée :  0.0018661022186279297\n",
      "Erreur finale : 0.6971018902567288\n"
     ]
    }
   ],
   "source": [
    "# BFGS avec Scipy\n",
    "print('méthode SLSQP avec Scipy (r = 0.1)')\n",
    "start_time = time.time()\n",
    "res = optimize.minimize(var, x0, method='SLSQP', jac=None, constraints=contraintes, options={'disp': True})\n",
    "print(\"Durée : \", time.time()-start_time)\n",
    "print(\"Erreur finale :\", np.linalg.norm(res.x-x_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d46920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résolution avec Casadi...\n",
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.14.11, running with linear solver MUMPS 5.4.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        6\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        6\n",
      "\n",
      "Total number of variables............................:        3\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        2\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  0.0000000e+00 1.00e+00 0.00e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  1.7067775e-02 0.00e+00 1.39e-17  -1.7 6.33e-01    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 1\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   1.7067775467775471e-02    1.7067775467775471e-02\n",
      "Dual infeasibility......:   1.3877787807814457e-17    1.3877787807814457e-17\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Overall NLP error.......:   1.3877787807814457e-17    1.3877787807814457e-17\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 2\n",
      "Number of objective gradient evaluations             = 2\n",
      "Number of equality constraint evaluations            = 2\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 2\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 1\n",
      "Total seconds in IPOPT                               = 0.002\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "      solver  :   t_proc      (avg)   t_wall      (avg)    n_eval\n",
      "       nlp_f  |   4.00us (  2.00us)   3.71us (  1.85us)         2\n",
      "       nlp_g  |  12.00us (  6.00us)   7.33us (  3.67us)         2\n",
      "  nlp_grad_f  |  10.00us (  3.33us)   9.83us (  3.28us)         3\n",
      "  nlp_hess_l  |   4.00us (  4.00us)   3.79us (  3.79us)         1\n",
      "   nlp_jac_g  |   9.00us (  3.00us)   8.54us (  2.85us)         3\n",
      "       total  |   1.75ms (  1.75ms)   1.90ms (  1.90ms)         1\n",
      "Durée :  0.0512540340423584\n",
      "[0.63284823 0.27858628 0.08856549]\n"
     ]
    }
   ],
   "source": [
    "# Résolution avec Casadi\n",
    "opti_cons = casadi.Opti();\n",
    "x = opti_cons.variable(3)\n",
    "\n",
    "opti_cons.minimize(var(x))\n",
    "opti_cons.subject_to(dot(unit.T, x) - 1 == 0)\n",
    "opti_cons.subject_to(dot(p.T, x) - r == 0)\n",
    "\n",
    "opti_cons.set_initial(x,x0)\n",
    "opti_cons.solver('ipopt')\n",
    "\n",
    "print(\"Résolution avec Casadi...\")\n",
    "start_time = time.time()\n",
    "sol = opti_cons.solve()\n",
    "print(\"Durée : \", time.time()-start_time)\n",
    "print(sol.value(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2cf17",
   "metadata": {},
   "source": [
    "On remarque que le temps de calcul avec Scipy et Casadi sont sensiblement les mêmes (de l'ordre de $10^{-3}$ s)\n",
    "\n",
    "De plus, les 2 méthodes aboutissent au même résultat, à savoir une variance de l'ordre de $10^{-2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f653564-ab89-4672-8140-a9efcffd3aab",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebfc741-5f25-4660-8061-b784c3fa65ea",
   "metadata": {},
   "source": [
    "$(a)$\n",
    "\n",
    "$\\rho$ est le coefficient de corrélation associé aux vecteurs $p_1$ et $p_2$. Il peut prendre des valeurs dans l'intervalle $[-1, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d8f98-b786-4645-8383-7fd11c6d3f10",
   "metadata": {},
   "source": [
    "$(b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad56110e-0959-4842-8e30-41211616a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition des variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bba1e",
   "metadata": {},
   "source": [
    "$(c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f241b",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a20864",
   "metadata": {},
   "source": [
    "$(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c0dbd",
   "metadata": {},
   "source": [
    "On ne résout qu'avec la méthode SLSQP de Scipy à présent pour plus de clarté, on n'utilise plus Casadi mais la réponse reste la même (cf. codes masqués ci-dessous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d16946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Méthode SLSQP avec Scipy (r = 0.2)\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10745197524851792\n",
      "            Iterations: 4\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 4\n",
      "Durée :  0.001753091812133789\n",
      "Erreur finale : 0.8553154235324663\n"
     ]
    }
   ],
   "source": [
    "# Pour r = 0.2\n",
    "r1 = 0.2\n",
    "\n",
    "contraintes1 = ({'type': 'eq', 'fun': lambda x:  unit.T@x -1},\n",
    "        {'type': 'eq', 'fun': lambda x: p.T@x - r1})\n",
    "\n",
    "print('  Méthode SLSQP avec Scipy (r = 0.2)')\n",
    "start_time = time.time()\n",
    "res = optimize.minimize(var, x0, method='SLSQP', jac=None, constraints=contraintes1, options={'disp': True})\n",
    "print(\"Durée : \", time.time()-start_time)\n",
    "print(\"Erreur finale :\", np.linalg.norm(res.x-x_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b36bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Méthode SLSQP avec Scipy (r = 0.15)\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04956735987023322\n",
      "            Iterations: 4\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 4\n",
      "Durée :  0.0017039775848388672\n",
      "Erreur finale : 0.634496924210732\n"
     ]
    }
   ],
   "source": [
    "# pour r = 0.15\n",
    "r2 = 0.15\n",
    "\n",
    "contraintes2 = ({'type': 'eq', 'fun': lambda x:  unit.T@x -1},\n",
    "        {'type': 'eq', 'fun': lambda x: p.T@x - r2})\n",
    "\n",
    "print('  Méthode SLSQP avec Scipy (r = 0.15)')\n",
    "start_time = time.time()\n",
    "res = optimize.minimize(var, x0, method='SLSQP', jac=None, constraints=contraintes2, options={'disp': True})\n",
    "print(\"Durée : \", time.time()-start_time)\n",
    "print(\"Erreur finale :\", np.linalg.norm(res.x-x_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8e4115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# équivalent avec Casadi pour r = 0.2\\n\\nopti_cons1 = casadi.Opti();\\nx1 = opti_cons1.variable(3)\\n\\nopti_cons1.minimize(var(x1))\\nopti_cons1.subject_to(dot(unit.T, x1) - 1 == 0)\\nopti_cons1.subject_to(dot(p.T, x1) - r1 == 0)\\n\\nopti_cons1.set_initial(x1,x0)\\nopti_cons1.solver(\\'ipopt\\')\\n\\nprint(\"\\n \\n  Résolution avec Casadi\")\\nstart_time = time.time()\\nsol = opti_cons1.solve()\\nprint(\"Durée : \", time.time()-start_time)\\nprint(sol.value(x1))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# équivalent avec Casadi pour r = 0.2\n",
    "\n",
    "opti_cons1 = casadi.Opti();\n",
    "x1 = opti_cons1.variable(3)\n",
    "\n",
    "opti_cons1.minimize(var(x1))\n",
    "opti_cons1.subject_to(dot(unit.T, x1) - 1 == 0)\n",
    "opti_cons1.subject_to(dot(p.T, x1) - r1 == 0)\n",
    "\n",
    "opti_cons1.set_initial(x1,x0)\n",
    "opti_cons1.solver('ipopt')\n",
    "\n",
    "print(\"\\n \\n  Résolution avec Casadi\")\n",
    "start_time = time.time()\n",
    "sol = opti_cons1.solve()\n",
    "print(\"Durée : \", time.time()-start_time)\n",
    "print(sol.value(x1))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d9637d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# équivalent avec Casadi pour r = 0.15\\n\\nopti_cons2 = casadi.Opti();\\nx2 = opti_cons2.variable(3)\\n\\nopti_cons2.minimize(var(x2))\\nopti_cons2.subject_to(dot(unit.T, x2) - 1 == 0)\\nopti_cons2.subject_to(dot(p.T, x2) - r2 == 0)\\n\\nopti_cons2.set_initial(x2,x0)\\nopti_cons2.solver(\\'ipopt\\')\\n\\nprint(\"\\n \\n  Résolution avec Casadi\")\\nstart_time = time.time()\\nsol = opti_cons2.solve()\\nprint(\"Durée : \", time.time()-start_time)\\nprint(sol.value(x2))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# équivalent avec Casadi pour r = 0.15\n",
    "\n",
    "opti_cons2 = casadi.Opti();\n",
    "x2 = opti_cons2.variable(3)\n",
    "\n",
    "opti_cons2.minimize(var(x2))\n",
    "opti_cons2.subject_to(dot(unit.T, x2) - 1 == 0)\n",
    "opti_cons2.subject_to(dot(p.T, x2) - r2 == 0)\n",
    "\n",
    "opti_cons2.set_initial(x2,x0)\n",
    "opti_cons2.solver('ipopt')\n",
    "\n",
    "print(\"\\n \\n  Résolution avec Casadi\")\n",
    "start_time = time.time()\n",
    "sol = opti_cons2.solve()\n",
    "print(\"Durée : \", time.time()-start_time)\n",
    "print(sol.value(x2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c613776",
   "metadata": {},
   "source": [
    "On voit qu'une variation assez faible du rendement $r$ a une importance élevée sur la variance (on passe de $10^{-2}$ à $10^{-1}$ sur une variation assez faible de rendement, de l'ordre de quelques centièmes)\n",
    "\n",
    "En revanche, on retrouve l'idée intuitive que pour un rendement d'investissement élevé (à prix équivalents), les allocations se concentrent davantage sur les actifs qui engendrent du bénéfice, mais donc le risque augmente aussi car on obtient des allocations réparties moins uniformément entre les différents actifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0dfc0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
